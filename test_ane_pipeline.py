#!/usr/bin/env python3
"""
Hybrid ANE-Accelerated TTS Pipeline Test Script

This script demonstrates the hybrid architecture for Kokoro TTS that splits
computation between CPU (text processing) and ANE (vocoder). This approach
maximizes performance by using the best compute unit for each task.

Architecture Overview:
1. CPU: Text → Phonemes → Spectrogram features (PyTorch)
2. ANE: Spectrogram features → Audio waveform (CoreML)

Key Benefits:
- LSTM/BERT components run on CPU where they're supported
- Vocoder runs on ANE for maximum throughput
- Clean separation of concerns for easier debugging
- Maintains full audio quality while improving performance

Prerequisites:
- KokoroVocoder.mlpackage (generated by export_vocoder.py)
- Original Kokoro PyTorch model for text processing
"""

import os
import time
import torch
import numpy as np
from pathlib import Path

# Optional imports with fallbacks
try:
    import soundfile as sf
    SOUNDFILE_AVAILABLE = True
except ImportError:
    SOUNDFILE_AVAILABLE = False
    print("ℹ️ soundfile not available - audio saving will be skipped")

# Check if CoreML conversion worked
COREML_MODEL_PATH = "KokoroVocoder.mlpackage"
COREML_AVAILABLE = os.path.exists(COREML_MODEL_PATH)

if COREML_AVAILABLE:
    try:
        import coremltools as ct
    except ImportError:
        COREML_AVAILABLE = False
        print("ℹ️ coremltools not available - using PyTorch only")

from kokoro import KModel, KPipeline

class HybridTTSPipeline:
    """
    Hybrid TTS pipeline that uses PyTorch for text processing and CoreML for vocoding.
    
    This class demonstrates the optimal architecture split:
    - PyTorch: Handles BERT, LSTM, and text encoding (CPU-optimized)
    - CoreML: Handles iSTFTNet vocoder (ANE-optimized)
    
    The pipeline maintains compatibility with the original Kokoro interface
    while providing significant performance improvements for the vocoder stage.
    """
    
    def __init__(self):
        """Initialize the hybrid pipeline with both PyTorch and CoreML components."""
        print("🚀 Initializing Hybrid ANE-Accelerated TTS Pipeline...")
        
        # Initialize PyTorch components for text processing
        print("📦 Loading PyTorch text processing components...")
        self.pytorch_model = KModel().to('cpu').eval()
        self.pipeline = KPipeline(lang_code='a', model=False)  # English pipeline
        print("✅ PyTorch components loaded")
        
        # Initialize CoreML vocoder if available
        if COREML_AVAILABLE:
            print("🍎 Loading CoreML vocoder...")
            try:
                self.coreml_vocoder = ct.models.MLModel(COREML_MODEL_PATH)
                self.use_coreml = True
                print("✅ CoreML vocoder loaded successfully")
                
                # Print vocoder specifications
                print("\n📋 CoreML Vocoder Info:")
                for input_spec in self.coreml_vocoder.get_spec().description.input:
                    print(f"  Input - {input_spec.name}: {input_spec.type}")
                for output_spec in self.coreml_vocoder.get_spec().description.output:
                    print(f"  Output - {output_spec.name}: {output_spec.type}")
                    
            except Exception as e:
                print(f"⚠️ CoreML vocoder loading failed: {e}")
                print("🔄 Falling back to PyTorch-only pipeline")
                self.use_coreml = False
        else:
            print("⚠️ CoreML vocoder not found, using PyTorch-only pipeline")
            self.use_coreml = False
            
        print(f"\n🎯 Pipeline Mode: {'Hybrid (PyTorch + CoreML)' if self.use_coreml else 'PyTorch Only'}")
    
    def extract_vocoder_inputs(self, text, voice='af_heart', speed=1.0):
        """
        Extract vocoder inputs using PyTorch text processing pipeline.
        
        This method runs the first part of the TTS pipeline (text → spectrogram)
        using the original PyTorch implementation, then extracts the inputs
        needed for the CoreML vocoder.
        
        Args:
            text: Input text to synthesize
            voice: Voice ID to use
            speed: Speech rate multiplier
            
        Returns:
            dict: Vocoder inputs (asr, f0_curve, n, s) or None if extraction fails
        """
        print(f"\n🔤 Processing text with PyTorch: '{text}'")
        
        try:
            # Load voice pack
            voice_pack = self.pipeline.load_voice(voice)
            
            # Process text through the pipeline to get phonemes
            phonemes = None
            for _, ps, _ in self.pipeline(text, voice, speed):
                phonemes = ps
                break
                
            if not phonemes:
                print("❌ Failed to extract phonemes")
                return None
                
            print(f"🔊 Phonemes: {phonemes}")
            
            # Get voice reference style
            ref_s = voice_pack[len(phonemes)-1]  # Reference style for this length
            
            # Run through the PyTorch model up to the vocoder stage
            # We need to extract the inputs that would normally go to the decoder
            input_ids = list(filter(lambda i: i is not None, 
                                  map(lambda p: self.pytorch_model.vocab.get(p), phonemes)))
            input_ids = torch.LongTensor([[0, *input_ids, 0]]).to(self.pytorch_model.device)
            ref_s = ref_s.to(self.pytorch_model.device)
            
            # Run forward pass up to decoder inputs
            with torch.no_grad():
                input_lengths = torch.full((input_ids.shape[0],), input_ids.shape[-1], 
                                         device=input_ids.device, dtype=torch.long)
                text_mask = torch.arange(input_lengths.max()).unsqueeze(0).expand(
                    input_lengths.shape[0], -1).type_as(input_lengths)
                text_mask = torch.gt(text_mask+1, input_lengths.unsqueeze(1)).to(self.pytorch_model.device)
                
                # BERT encoding
                bert_dur = self.pytorch_model.bert(input_ids, attention_mask=(~text_mask).int())
                d_en = self.pytorch_model.bert_encoder(bert_dur).transpose(-1, -2)
                s = ref_s[:, 128:]  # Style embedding
                
                # Prosody prediction
                d = self.pytorch_model.predictor.text_encoder(d_en, s, input_lengths, text_mask)
                x, _ = self.pytorch_model.predictor.lstm(d)
                duration = self.pytorch_model.predictor.duration_proj(x)
                duration = torch.sigmoid(duration).sum(axis=-1) / speed
                pred_dur = torch.round(duration).clamp(min=1).long().squeeze()
                
                # Duration alignment
                indices = torch.repeat_interleave(
                    torch.arange(input_ids.shape[1], device=self.pytorch_model.device), pred_dur)
                pred_aln_trg = torch.zeros((input_ids.shape[1], indices.shape[0]), 
                                         device=self.pytorch_model.device)
                pred_aln_trg[indices, torch.arange(indices.shape[0])] = 1
                pred_aln_trg = pred_aln_trg.unsqueeze(0).to(self.pytorch_model.device)
                
                # Generate F0 and noise predictions
                en = d.transpose(-1, -2) @ pred_aln_trg
                F0_pred, N_pred = self.pytorch_model.predictor.F0Ntrain(en, s)
                
                # Text encoder features
                t_en = self.pytorch_model.text_encoder(input_ids, input_lengths, text_mask)
                asr = t_en @ pred_aln_trg
                
                # Extract vocoder inputs
                vocoder_inputs = {
                    'asr': asr.cpu().numpy().astype(np.float32),
                    'f0_curve': F0_pred.cpu().numpy().astype(np.float32), 
                    'n': N_pred.cpu().numpy().astype(np.float32),
                    's': ref_s[:, :128].cpu().numpy().astype(np.float32)  # Style embedding
                }
                
                print("✅ Successfully extracted vocoder inputs")
                print(f"  - ASR features: {vocoder_inputs['asr'].shape}")
                print(f"  - F0 curve: {vocoder_inputs['f0_curve'].shape}")
                print(f"  - Noise: {vocoder_inputs['n'].shape}")
                print(f"  - Style: {vocoder_inputs['s'].shape}")
                
                return vocoder_inputs
                
        except Exception as e:
            print(f"❌ Error extracting vocoder inputs: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def run_coreml_vocoder(self, vocoder_inputs):
        """
        Run the CoreML vocoder on extracted inputs.
        
        Args:
            vocoder_inputs: Dictionary of inputs for the vocoder
            
        Returns:
            numpy.ndarray: Generated audio waveform or None if failed
        """
        if not self.use_coreml:
            print("❌ CoreML vocoder not available")
            return None
            
        print("🍎 Running CoreML vocoder on ANE...")
        
        try:
            start_time = time.time()
            result = self.coreml_vocoder.predict(vocoder_inputs)
            end_time = time.time()
            
            # Extract audio from results
            audio_key = list(result.keys())[0]  # Get the output key
            audio = result[audio_key]
            
            print(f"✅ CoreML vocoder completed in {end_time - start_time:.3f}s")
            print(f"  - Audio shape: {audio.shape}")
            print(f"  - Audio range: [{audio.min():.3f}, {audio.max():.3f}]")
            
            # Flatten if needed and convert to 1D audio
            if audio.ndim > 1:
                audio = audio.squeeze()
            
            return audio
            
        except Exception as e:
            print(f"❌ CoreML vocoder failed: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def run_pytorch_fallback(self, text, voice='af_heart', speed=1.0):
        """
        Run the complete pipeline using PyTorch only as a fallback.
        
        Args:
            text: Input text
            voice: Voice ID
            speed: Speech rate
            
        Returns:
            numpy.ndarray: Generated audio waveform
        """
        print("🔄 Running PyTorch fallback pipeline...")
        
        try:
            start_time = time.time()
            
            # Use the original KPipeline for full synthesis
            for _, phonemes, _ in self.pipeline(text, voice, speed):
                voice_pack = self.pipeline.load_voice(voice)
                ref_s = voice_pack[len(phonemes)-1]
                
                audio = self.pytorch_model(phonemes, ref_s, speed)
                
                end_time = time.time()
                print(f"✅ PyTorch fallback completed in {end_time - start_time:.3f}s")
                
                return audio.numpy()
                
        except Exception as e:
            print(f"❌ PyTorch fallback failed: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def synthesize(self, text, voice='af_heart', speed=1.0):
        """
        Main synthesis method that orchestrates the hybrid pipeline.
        
        Args:
            text: Text to synthesize
            voice: Voice ID to use
            speed: Speech rate multiplier
            
        Returns:
            tuple: (audio_array, sample_rate) or (None, None) if failed
        """
        print(f"\n🎵 Synthesizing: '{text}' (voice: {voice}, speed: {speed}x)")
        
        if self.use_coreml:
            # Try hybrid pipeline first
            vocoder_inputs = self.extract_vocoder_inputs(text, voice, speed)
            if vocoder_inputs:
                audio = self.run_coreml_vocoder(vocoder_inputs)
                if audio is not None:
                    return audio, 24000  # Kokoro uses 24kHz
            
            print("⚠️ Hybrid pipeline failed, falling back to PyTorch")
        
        # Fallback to PyTorch-only
        audio = self.run_pytorch_fallback(text, voice, speed)
        if audio is not None:
            return audio, 24000
        
        print("❌ All synthesis methods failed")
        return None, None

def check_ane_usage():
    """
    Check if the Apple Neural Engine is being used.
    
    This function provides various methods to verify ANE utilization,
    from simple model inspection to system-level monitoring.
    """
    print("\n🔍 Checking ANE Usage...")
    
    if not COREML_AVAILABLE:
        print("❌ CoreML model not available - cannot check ANE usage")
        return
    
    try:
        model = ct.models.MLModel(COREML_MODEL_PATH)
        
        # Check compute units configuration
        compute_units = model.compute_units
        print(f"📊 Model compute units: {compute_units}")
        
        if compute_units == ct.ComputeUnit.ALL:
            print("✅ Model allows ANE usage (compute_units=ALL)")
        elif compute_units == ct.ComputeUnit.CPU_AND_NE:
            print("✅ Model configured for CPU+ANE")
        else:
            print(f"⚠️ Model may not use ANE (compute_units={compute_units})")
        
        # Print performance recommendations
        print("\n💡 To verify ANE usage during runtime:")
        print("1. Use Instruments with Core ML template")
        print("2. Monitor 'Neural Engine' activity during inference")
        print("3. Run: sudo powermetrics -i 1000 --samplers ane | grep 'ANE Power'")
        print("4. Check for H11ANEServicesThread activity in Activity Monitor")
        
    except Exception as e:
        print(f"❌ Error checking ANE usage: {e}")

def run_performance_test(pipeline, test_texts):
    """
    Run performance benchmarks comparing different pipeline modes.
    
    Args:
        pipeline: HybridTTSPipeline instance
        test_texts: List of test texts to synthesize
    """
    print("\n⚡ Running Performance Tests...")
    
    results = []
    
    for i, text in enumerate(test_texts):
        print(f"\n📝 Test {i+1}: '{text[:50]}{'...' if len(text) > 50 else ''}'")
        
        start_time = time.time()
        audio, sample_rate = pipeline.synthesize(text)
        end_time = time.time()
        
        if audio is not None:
            duration = end_time - start_time
            audio_length = len(audio) / sample_rate
            rtf = duration / audio_length  # Real-time factor
            
            result = {
                'text': text,
                'synthesis_time': duration,
                'audio_length': audio_length,
                'rtf': rtf,
                'success': True
            }
            
            print(f"  ⏱️  Synthesis time: {duration:.3f}s")
            print(f"  🎵 Audio length: {audio_length:.3f}s")
            print(f"  🚀 Real-time factor: {rtf:.3f}x")
            
        else:
            result = {
                'text': text,
                'success': False
            }
            print("  ❌ Synthesis failed")
        
        results.append(result)
    
    # Summary
    successful_results = [r for r in results if r['success']]
    if successful_results:
        avg_rtf = sum(r['rtf'] for r in successful_results) / len(successful_results)
        print(f"\n📊 Performance Summary:")
        print(f"  - Successful syntheses: {len(successful_results)}/{len(results)}")
        print(f"  - Average RTF: {avg_rtf:.3f}x")
        if avg_rtf < 1.0:
            print("  ✅ Pipeline is faster than real-time!")
        else:
            print("  ⚠️ Pipeline is slower than real-time")

def main():
    """Main execution function for the hybrid pipeline test."""
    print("🎯 Hybrid ANE-Accelerated TTS Pipeline Test")
    print("=" * 50)
    
    # Initialize pipeline
    try:
        pipeline = HybridTTSPipeline()
    except Exception as e:
        print(f"❌ Failed to initialize pipeline: {e}")
        return
    
    # Check ANE usage capabilities
    check_ane_usage()
    
    # Test texts of varying complexity
    test_texts = [
        "Hello world!",
        "The quick brown fox jumps over the lazy dog.",
        "This is a longer sentence that will test the performance of our hybrid pipeline architecture.",
        "Kokoro is a high-quality text-to-speech system that can generate natural sounding speech."
    ]
    
    # Run performance tests
    run_performance_test(pipeline, test_texts)
    
    # Generate sample outputs
    print("\n🎵 Generating Sample Audio Files...")
    output_dir = Path("outputs")
    output_dir.mkdir(exist_ok=True)
    
    for i, text in enumerate(test_texts[:2]):  # Just first two for samples
        print(f"\n📝 Generating sample {i+1}: '{text}'")
        
        audio, sample_rate = pipeline.synthesize(text, voice='af_heart', speed=1.0)
        
        if audio is not None:
            output_path = output_dir / f"sample_{i+1:02d}.wav"
            if SOUNDFILE_AVAILABLE:
                sf.write(output_path, audio, sample_rate)
                print(f"  💾 Saved: {output_path}")
            else:
                print(f"  ⚠️ Would save to: {output_path} (soundfile not available)")
                print(f"  📊 Audio info: {audio.shape}, range [{audio.min():.3f}, {audio.max():.3f}]")
        else:
            print("  ❌ Failed to generate audio")
    
    print(f"\n🎉 Pipeline test completed!")
    print(f"📁 Sample audio files saved in: {output_dir}")
    
    if pipeline.use_coreml:
        print("\n🔥 Next Steps for ANE Usage Verification:")
        print("1. Run this script while monitoring with Instruments")
        print("2. Use 'sudo powermetrics -i 1000 --samplers ane' in another terminal")
        print("3. Look for Neural Engine activity during CoreML inference")

if __name__ == "__main__":
    main()